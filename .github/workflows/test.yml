name: Automated Testing Pipeline

on:
  push:
    branches: [ main, develop, 'feature/**' ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - unit
        - integration
        - performance
        - cross-browser

env:
  NODE_VERSION: '18'
  PLAYWRIGHT_BROWSERS_PATH: ${{ github.workspace }}/pw-browsers

jobs:
  # Unit Tests - Fast feedback
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    if: ${{ !contains(github.event.inputs.test_type, 'cross-browser') || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'unit' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Run unit tests
      run: |
        npm run test -- --reporter=verbose --coverage
        
    - name: Upload coverage reports
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage/lcov.info
        flags: unit-tests
        name: unit-test-coverage
        
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: unit-test-results
        path: |
          coverage/
          test-results/
          
  # Integration Tests - Module interactions
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    if: ${{ !contains(github.event.inputs.test_type, 'cross-browser') || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'integration' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Run integration tests
      run: |
        npm run test:integration -- --reporter=verbose
        
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: test-results/
        
  # Performance Tests - Baseline comparison
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: ${{ !contains(github.event.inputs.test_type, 'cross-browser') || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'performance' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Run performance benchmarks
      run: |
        npm run test tests/performance/ -- --reporter=verbose
        
    - name: Compare with baselines
      run: |
        echo "Performance test results:"
        if [ -f test-results/performance-report.json ]; then
          cat test-results/performance-report.json
        fi
        
    - name: Upload performance results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: |
          test-results/
          coverage/
          
  # Cross-Browser Tests - Multiple browsers and devices
  cross-browser-tests:
    name: Cross-Browser Tests (${{ matrix.browser }})
    runs-on: ${{ matrix.os }}
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'cross-browser' || github.event_name == 'schedule' }}
    
    strategy:
      fail-fast: false
      matrix:
        include:
          # Desktop browsers
          - browser: chromium
            os: ubuntu-latest
          - browser: firefox
            os: ubuntu-latest
          - browser: webkit
            os: ubuntu-latest
          # Windows testing
          - browser: chromium
            os: windows-latest
          # macOS testing (for Safari)
          - browser: webkit
            os: macos-latest
            
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Cache Playwright browsers
      uses: actions/cache@v4
      id: playwright-cache
      with:
        path: ${{ env.PLAYWRIGHT_BROWSERS_PATH }}
        key: playwright-browsers-${{ matrix.browser }}-${{ runner.os }}
        
    - name: Install Playwright browsers
      if: steps.playwright-cache.outputs.cache-hit != 'true'
      run: npx playwright install ${{ matrix.browser }} --with-deps
      
    - name: Start development server
      run: |
        npm run dev &
        npx wait-on http://localhost:3000 --timeout 30000
        
    - name: Run cross-browser tests
      run: |
        npx playwright test --project=${{ matrix.browser }}-desktop
      env:
        CI: true
        PLAYWRIGHT_BROWSERS_PATH: ${{ env.PLAYWRIGHT_BROWSERS_PATH }}
        
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: playwright-results-${{ matrix.browser }}-${{ matrix.os }}
        path: |
          test-results/
          playwright-report/
          
  # Test Results Summary
  test-summary:
    name: Test Results Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests, cross-browser-tests]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all test results
      uses: actions/download-artifact@v4
      with:
        path: all-test-results/
        
    - name: Generate test summary
      run: |
        echo "# Test Results Summary" > test-summary.md
        echo "" >> test-summary.md
        echo "## Unit Tests" >> test-summary.md
        if [ -d "all-test-results/unit-test-results" ]; then
          echo "✅ Unit tests completed" >> test-summary.md
        else
          echo "❌ Unit tests failed or skipped" >> test-summary.md
        fi
        echo "" >> test-summary.md
        
        echo "## Integration Tests" >> test-summary.md
        if [ -d "all-test-results/integration-test-results" ]; then
          echo "✅ Integration tests completed" >> test-summary.md
        else
          echo "❌ Integration tests failed or skipped" >> test-summary.md
        fi
        echo "" >> test-summary.md
        
        echo "## Performance Tests" >> test-summary.md
        if [ -d "all-test-results/performance-test-results" ]; then
          echo "✅ Performance tests completed" >> test-summary.md
        else
          echo "❌ Performance tests failed or skipped" >> test-summary.md
        fi
        echo "" >> test-summary.md
        
        echo "## Cross-Browser Tests" >> test-summary.md
        BROWSER_COUNT=$(find all-test-results/ -name "playwright-results-*" -type d | wc -l)
        if [ $BROWSER_COUNT -gt 0 ]; then
          echo "✅ Cross-browser tests completed on $BROWSER_COUNT browser(s)" >> test-summary.md
        else
          echo "❌ Cross-browser tests failed or skipped" >> test-summary.md
        fi
        
        cat test-summary.md
        
    - name: Upload test summary
      uses: actions/upload-artifact@v4
      with:
        name: test-summary
        path: |
          test-summary.md
          all-test-results/
          
    - name: Comment PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('test-summary.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });
          
  # Quality Gates - Fail build if quality metrics not met
  quality-gate:
    name: Quality Gate
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests]
    if: always() && !contains(github.event.inputs.test_type, 'cross-browser')
    
    steps:
    - name: Download test results
      uses: actions/download-artifact@v4
      with:
        name: unit-test-results
        path: test-results/
        
    - name: Check coverage threshold
      run: |
        if [ -f "test-results/coverage/coverage-summary.json" ]; then
          COVERAGE=$(cat test-results/coverage/coverage-summary.json | jq '.total.statements.pct')
          echo "Coverage: $COVERAGE%"
          
          if (( $(echo "$COVERAGE < 80" | bc -l) )); then
            echo "Coverage $COVERAGE% is below threshold of 80%"
            exit 1
          fi
          
          echo "Coverage check passed: $COVERAGE%"
        else
          echo "Coverage report not found"
          exit 1
        fi
        
    - name: Quality gate passed
      run: |
        echo "✅ All quality gates passed!"
        echo "- Code coverage >= 80%"
        echo "- All unit tests passing"
        echo "- All integration tests passing"
        echo "- Performance benchmarks within limits"